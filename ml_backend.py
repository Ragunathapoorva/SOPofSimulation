# Security IoT Attack Simulation & ML Backend
# FGSM and PGD Adversarial Attack Implementation

import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
import hashlib
import uuid

# For ML model training (if numpy-based, no external deps initially)

class FGSMAttackSimulator:
    """
    Fast Gradient Sign Method (FGSM) Implementation
    Generates adversarial examples against neural network classifiers
    """
    
    def __init__(self, epsilon: float = 0.3, model_weights: np.ndarray = None):
        """
        Initialize FGSM Attack Simulator
        
        Args:
            epsilon: Perturbation magnitude (0.0 to 1.0)
            model_weights: Pre-trained model weights (optional)
        """
        self.epsilon = epsilon
        self.model_weights = model_weights
        self.attack_log = []
        
    def generate_perturbation(self, input_data: np.ndarray, 
                             target_label: int, 
                             loss_gradient: np.ndarray) -> np.ndarray:
        """
        Generate FGSM perturbation
        
        Args:
            input_data: Original input (image or sensor data)
            target_label: Target class for misclassification
            loss_gradient: Gradient of loss w.r.t. input
            
        Returns:
            Adversarial perturbation
        """
        # Compute sign of gradient
        perturbation = self.epsilon * np.sign(loss_gradient)
        
        # Clip to valid range
        perturbation = np.clip(perturbation, -self.epsilon, self.epsilon)
        
        return perturbation
    
    def execute_attack(self, input_sample: np.ndarray, 
                      true_label: int,
                      model_predict_fn=None) -> Dict:
        """
        Execute FGSM attack on input sample
        
        Args:
            input_sample: Input to attack
            true_label: True classification label
            model_predict_fn: Model prediction function
            
        Returns:
            Attack results dictionary
        """
        attack_id = str(uuid.uuid4())[:8]
        timestamp = datetime.now().isoformat()
        
        # Normalize input
        x_normalized = input_sample.astype(np.float32) / 255.0
        
        # Simulate gradient computation
        gradient = np.random.randn(*x_normalized.shape) * 0.5
        
        # Generate perturbation
        perturbation = self.generate_perturbation(x_normalized, 
                                                  true_label, 
                                                  gradient)
        
        # Create adversarial example
        x_adversarial = x_normalized + perturbation
        x_adversarial = np.clip(x_adversarial, 0, 1)
        
        # Simulate attack success
        attack_success = np.random.random() > 0.3  # 70% success rate
        
        result = {
            'attack_id': attack_id,
            'timestamp': timestamp,
            'epsilon': self.epsilon,
            'perturbation_magnitude': np.linalg.norm(perturbation),
            'attack_success': attack_success,
            'success_rate': np.random.uniform(0.65, 0.95) if attack_success else np.random.uniform(0.1, 0.35),
            'detection_time': np.random.uniform(2.5, 4.5),
            'false_positive_rate': np.random.uniform(0.5, 2.0),
            'recovery_time': np.random.uniform(15, 30)
        }
        
        self.attack_log.append(result)
        return result


class PGDAttackSimulator:
    """
    Projected Gradient Descent (PGD) Attack Implementation
    Iterative adversarial attack with multiple gradient steps
    """
    
    def __init__(self, epsilon: float = 0.3, 
                 alpha: float = 0.01, 
                 iterations: int = 20):
        """
        Initialize PGD Attack Simulator
        
        Args:
            epsilon: Total perturbation budget
            alpha: Step size for each gradient step
            iterations: Number of iterations
        """
        self.epsilon = epsilon
        self.alpha = alpha
        self.iterations = iterations
        self.attack_log = []
    
    def execute_attack(self, input_sample: np.ndarray,
                      true_label: int,
                      model_predict_fn=None) -> Dict:
        """
        Execute PGD attack on input sample
        
        Args:
            input_sample: Input to attack
            true_label: True label
            model_predict_fn: Model prediction function
            
        Returns:
            Attack results dictionary
        """
        attack_id = str(uuid.uuid4())[:8]
        timestamp = datetime.now().isoformat()
        
        # Initialize perturbation
        x_normalized = input_sample.astype(np.float32) / 255.0
        delta = np.random.uniform(-self.epsilon, self.epsilon, 
                                 x_normalized.shape).astype(np.float32)
        
        iteration_logs = []
        
        # Iterative attack steps
        for iter_num in range(self.iterations):
            # Simulate gradient
            gradient = np.random.randn(*x_normalized.shape) * 0.3
            
            # Gradient step
            delta += self.alpha * np.sign(gradient)
            
            # Project to epsilon ball
            delta = np.clip(delta, -self.epsilon, self.epsilon)
            
            # Clip adversarial example to valid range
            x_adversarial = np.clip(x_normalized + delta, 0, 1)
            
            iter_log = {
                'iteration': iter_num + 1,
                'perturbation_norm': np.linalg.norm(delta),
                'success_probability': 0.1 + (iter_num / self.iterations) * 0.7
            }
            iteration_logs.append(iter_log)
        
        # Final adversarial example
        attack_success = np.random.random() > 0.25  # 75% success rate
        
        result = {
            'attack_id': attack_id,
            'timestamp': timestamp,
            'epsilon': self.epsilon,
            'alpha': self.alpha,
            'iterations': self.iterations,
            'final_perturbation_magnitude': np.linalg.norm(delta),
            'attack_success': attack_success,
            'success_rate': np.random.uniform(0.70, 0.98) if attack_success else np.random.uniform(0.15, 0.40),
            'detection_time': np.random.uniform(4.0, 7.0),
            'false_positive_rate': np.random.uniform(0.8, 2.5),
            'recovery_time': np.random.uniform(20, 45),
            'iteration_logs': iteration_logs
        }
        
        self.attack_log.append(result)
        return result


class AdversarialTrainingModel:
    """
    Train models to be robust against adversarial examples using FGSM and PGD
    """
    
    def __init__(self, model_name: str = "IoT_Defense_Model"):
        self.model_name = model_name
        self.model_id = str(uuid.uuid4())[:12]
        self.training_history = []
        self.validation_history = []
        self.created_at = datetime.now().isoformat()
    
    def train_fgsm_robust_model(self, 
                               dataset_size: int = 1000,
                               epochs: int = 50,
                               batch_size: int = 32,
                               epsilon: float = 0.3) -> Dict:
        """
        Train model with FGSM adversarial training
        
        Args:
            dataset_size: Number of training samples
            epochs: Training epochs
            batch_size: Batch size
            epsilon: FGSM epsilon for adversarial examples
            
        Returns:
            Training history
        """
        fgsm_simulator = FGSMAttackSimulator(epsilon=epsilon)
        
        history = {
            'model_id': self.model_id,
            'model_name': self.model_name,
            'training_type': 'FGSM_Adversarial',
            'epochs': epochs,
            'batch_size': batch_size,
            'dataset_size': dataset_size,
            'epsilon': epsilon,
            'start_time': datetime.now().isoformat(),
            'epoch_logs': []
        }
        
        # Simulate training process
        for epoch in range(epochs):
            # Generate synthetic training data
            train_data = np.random.randn(dataset_size, 28, 28)  # MNIST-like
            
            # Simulate loss and accuracy
            clean_loss = 2.5 * np.exp(-epoch / 15)  # Exponential decay
            adversarial_loss = 2.0 * np.exp(-epoch / 12)
            
            clean_accuracy = 50 + (epoch / epochs) * 45
            adversarial_accuracy = 40 + (epoch / epochs) * 50
            robust_accuracy = (clean_accuracy + adversarial_accuracy) / 2
            
            epoch_log = {
                'epoch': epoch + 1,
                'clean_loss': round(clean_loss, 4),
                'adversarial_loss': round(adversarial_loss, 4),
                'clean_accuracy': round(clean_accuracy, 2),
                'adversarial_accuracy': round(adversarial_accuracy, 2),
                'robust_accuracy': round(robust_accuracy, 2)
            }
            
            history['epoch_logs'].append(epoch_log)
        
        history['end_time'] = datetime.now().isoformat()
        history['training_status'] = 'completed'
        
        self.training_history.append(history)
        return history
    
    def train_pgd_robust_model(self,
                              dataset_size: int = 1000,
                              epochs: int = 50,
                              batch_size: int = 32,
                              epsilon: float = 0.3,
                              pgd_iterations: int = 20) -> Dict:
        """
        Train model with PGD adversarial training
        
        Args:
            dataset_size: Number of training samples
            epochs: Training epochs
            batch_size: Batch size
            epsilon: PGD epsilon for adversarial examples
            pgd_iterations: Number of PGD iterations
            
        Returns:
            Training history
        """
        history = {
            'model_id': self.model_id,
            'model_name': self.model_name,
            'training_type': 'PGD_Adversarial',
            'epochs': epochs,
            'batch_size': batch_size,
            'dataset_size': dataset_size,
            'epsilon': epsilon,
            'pgd_iterations': pgd_iterations,
            'start_time': datetime.now().isoformat(),
            'epoch_logs': []
        }
        
        # Simulate training process
        for epoch in range(epochs):
            # Simulate loss and accuracy with PGD training (typically better robustness)
            clean_loss = 2.3 * np.exp(-epoch / 18)
            pgd_loss = 1.8 * np.exp(-epoch / 14)
            
            clean_accuracy = 48 + (epoch / epochs) * 48
            pgd_accuracy = 42 + (epoch / epochs) * 52
            robust_accuracy = (clean_accuracy + pgd_accuracy) / 2
            
            epoch_log = {
                'epoch': epoch + 1,
                'clean_loss': round(clean_loss, 4),
                'pgd_loss': round(pgd_loss, 4),
                'clean_accuracy': round(clean_accuracy, 2),
                'pgd_accuracy': round(pgd_accuracy, 2),
                'robust_accuracy': round(robust_accuracy, 2)
            }
            
            history['epoch_logs'].append(epoch_log)
        
        history['end_time'] = datetime.now().isoformat()
        history['training_status'] = 'completed'
        
        self.training_history.append(history)
        return history
    
    def validate_model(self, 
                      test_dataset_size: int = 500) -> Dict:
        """
        Validate trained model
        
        Args:
            test_dataset_size: Size of validation dataset
            
        Returns:
            Validation results
        """
        validation_result = {
            'model_id': self.model_id,
            'validation_timestamp': datetime.now().isoformat(),
            'test_samples': test_dataset_size,
            'clean_accuracy': round(np.random.uniform(0.85, 0.96), 4),
            'adversarial_accuracy_fgsm': round(np.random.uniform(0.72, 0.88), 4),
            'adversarial_accuracy_pgd': round(np.random.uniform(0.68, 0.84), 4),
            'robustness_score': round(np.random.uniform(0.75, 0.92), 4),
            'recommendations': [
                "Model shows good robustness against FGSM attacks",
                "Consider increasing training with more adversarial examples",
                "Test against real-world IoT sensor data",
                "Monitor model drift in production"
            ]
        }
        
        self.validation_history.append(validation_result)
        return validation_result
    
    def get_model_summary(self) -> Dict:
        """Get comprehensive model summary"""
        total_epochs = sum(len(h['epoch_logs']) for h in self.training_history)
        
        return {
            'model_id': self.model_id,
            'model_name': self.model_name,
            'created_at': self.created_at,
            'training_sessions': len(self.training_history),
            'total_epochs': total_epochs,
            'validation_sessions': len(self.validation_history),
            'current_status': 'ready_for_deployment',
            'latest_accuracy': self.validation_history[-1]['robustness_score'] if self.validation_history else 0
        }


class AttackFlowSimulator:
    """
    Simulate complete attack flow with multiple stages
    """
    
    def __init__(self):
        self.simulations = []
    
    def simulate_attack_flow(self,
                            attack_type: str,
                            target_device: str,
                            mitigation_strategy: str,
                            duration_seconds: int = 120) -> Dict:
        """
        Simulate complete attack flow
        
        Args:
            attack_type: Type of attack (DDoS, Malware, FGSM, etc.)
            target_device: Target device type
            mitigation_strategy: Applied mitigation
            duration_seconds: Simulation duration in seconds
            
        Returns:
            Detailed simulation results
        """
        simulation_id = str(uuid.uuid4())[:12]
        start_time = datetime.now().isoformat()
        
        # Calculate base metrics based on attack and mitigation
        mitigation_effectiveness = {
            'No Mitigation': 0,
            'Proactive Monitoring': 35,
            'Adversarial Training': 68,
            'Network Segmentation': 52,
            'Zero Trust Architecture': 71,
            'AI-Powered Detection': 76,
            'Multi-Layered Defense': 84,
            'Full Defense Suite': 92
        }
        
        attack_base_success = {
            'DDoS': 85, 'Malware': 78, 'Spoofing': 72,
            'Man-in-the-Middle': 65, 'Ransomware': 58,
            'Zero-Day Exploit': 42, 'FGSM Attack': 72,
            'PGD Attack': 78, 'Botnet Recruitment': 68,
            'Firmware Injection': 51, 'SQL Injection': 68,
            'Brute Force': 45
        }
        
        effectiveness = mitigation_effectiveness.get(mitigation_strategy, 0)
        base_success = attack_base_success.get(attack_type, 60)
        
        # Calculate final metrics
        final_success_rate = max(5, base_success * (1 - effectiveness / 100))
        detection_time = max(0.5, np.random.uniform(2, 20) * (1 - effectiveness / 150))
        false_positive_rate = max(0.5, min(15, 10 - (effectiveness / 10)))
        recovery_time = max(5, 60 - (effectiveness * 0.5))
        security_score = min(100, 50 + effectiveness / 2 + (100 - final_success_rate) / 2)
        
        # Generate attack flow stages
        stages = []
        stage_durations = [duration_seconds * 0.2, duration_seconds * 0.3, 
                          duration_seconds * 0.3, duration_seconds * 0.1, 
                          duration_seconds * 0.1]
        
        stage_names = ['Reconnaissance', 'Exploitation', 'Payload Delivery', 
                      'Detection', 'Recovery']
        
        for i, (stage_name, stage_duration) in enumerate(zip(stage_names, stage_durations)):
            stage = {
                'stage': stage_name,
                'duration_seconds': int(stage_duration),
                'progress': (i + 1) / len(stage_names) * 100,
                'status': 'completed' if i < 3 else ('detected' if i == 3 else 'recovered'),
                'description': self._get_stage_description(stage_name, attack_type)
            }
            stages.append(stage)
        
        simulation_result = {
            'simulation_id': simulation_id,
            'start_time': start_time,
            'end_time': datetime.now().isoformat(),
            'attack_type': attack_type,
            'target_device': target_device,
            'mitigation_strategy': mitigation_strategy,
            'duration_seconds': duration_seconds,
            'attack_success': np.random.random() < (final_success_rate / 100),
            'success_rate': round(final_success_rate, 2),
            'detection_time': round(detection_time, 2),
            'false_positive_rate': round(false_positive_rate, 2),
            'recovery_time': round(recovery_time, 2),
            'security_score': round(security_score, 2),
            'stages': stages,
            'mitigation_effectiveness': effectiveness,
            'recommendations': self._generate_recommendations(attack_type, effectiveness, final_success_rate)
        }
        
        self.simulations.append(simulation_result)
        return simulation_result
    
    def _get_stage_description(self, stage: str, attack_type: str) -> str:
        descriptions = {
            'Reconnaissance': f"Attacker scanning {attack_type} vectors and gathering network intelligence",
            'Exploitation': f"Attempting to exploit vulnerabilities using {attack_type}",
            'Payload Delivery': f"Delivering malicious payload through compromised channels",
            'Detection': f"Security systems detecting anomalous behavior from {attack_type}",
            'Recovery': "System recovering from attack and restoring to normal state"
        }
        return descriptions.get(stage, "Attack stage in progress")
    
    def _generate_recommendations(self, attack_type: str, 
                                 effectiveness: int,
                                 success_rate: float) -> List[str]:
        recommendations = []
        
        if success_rate > 50:
            recommendations.append(f"‚ö†Ô∏è High success rate against {attack_type} - upgrade defenses")
        
        if effectiveness < 50:
            recommendations.append("üî¥ Current mitigation insufficient - deploy multi-layered defense")
        elif effectiveness < 75:
            recommendations.append("üü° Moderate protection - consider AI-powered detection")
        else:
            recommendations.append("üü¢ Good protection level - maintain current strategy")
        
        recommendations.append(f"üìä Deploy {attack_type}-specific detection rules")
        recommendations.append("üîÑ Enable continuous monitoring and threat hunting")
        
        return recommendations


# Example usage and API endpoints simulator
def create_api_response(endpoint: str, method: str, data: Dict = None) -> Dict:
    """
    Simulate API responses for web frontend
    """
    if endpoint == '/api/fgsm/attack' and method == 'POST':
        simulator = FGSMAttackSimulator(epsilon=data.get('epsilon', 0.3))
        return simulator.execute_attack(
            np.random.randn(28, 28),
            data.get('target_label', 0)
        )
    
    elif endpoint == '/api/pgd/attack' and method == 'POST':
        simulator = PGDAttackSimulator(
            epsilon=data.get('epsilon', 0.3),
            iterations=data.get('iterations', 20)
        )
        return simulator.execute_attack(
            np.random.randn(28, 28),
            data.get('target_label', 0)
        )
    
    elif endpoint == '/api/train/fgsm' and method == 'POST':
        model = AdversarialTrainingModel(data.get('model_name', 'FGSM_Model'))
        return model.train_fgsm_robust_model(
            dataset_size=data.get('dataset_size', 1000),
            epochs=data.get('epochs', 50),
            epsilon=data.get('epsilon', 0.3)
        )
    
    elif endpoint == '/api/train/pgd' and method == 'POST':
        model = AdversarialTrainingModel(data.get('model_name', 'PGD_Model'))
        return model.train_pgd_robust_model(
            dataset_size=data.get('dataset_size', 1000),
            epochs=data.get('epochs', 50),
            epsilon=data.get('epsilon', 0.3),
            pgd_iterations=data.get('pgd_iterations', 20)
        )
    
    elif endpoint == '/api/simulate/attack' and method == 'POST':
        flow_sim = AttackFlowSimulator()
        return flow_sim.simulate_attack_flow(
            attack_type=data.get('attack_type', 'DDoS'),
            target_device=data.get('target_device', 'Server'),
            mitigation_strategy=data.get('mitigation', 'AI-Powered Detection'),
            duration_seconds=data.get('duration', 120)
        )
    
    return {'error': 'Unknown endpoint'}


# Testing
if __name__ == '__main__':
    print("=== IoT Security Attack Simulation Backend ===\n")
    
    # Test FGSM
    print("1. Testing FGSM Attack Simulator...")
    fgsm_sim = FGSMAttackSimulator(epsilon=0.3)
    fgsm_result = fgsm_sim.execute_attack(np.random.randn(28, 28), 0)
    print(json.dumps(fgsm_result, indent=2))
    
    # Test PGD
    print("\n2. Testing PGD Attack Simulator...")
    pgd_sim = PGDAttackSimulator(epsilon=0.3, iterations=20)
    pgd_result = pgd_sim.execute_attack(np.random.randn(28, 28), 0)
    print(json.dumps({k: v for k, v in pgd_result.items() if k != 'iteration_logs'}, indent=2))
    
    # Test Training
    print("\n3. Testing Adversarial Training (FGSM)...")
    model = AdversarialTrainingModel("IoT_Defense_V1")
    training_history = model.train_fgsm_robust_model(epochs=10)
    print(f"Training completed. Final accuracy: {training_history['epoch_logs'][-1]['robust_accuracy']}%")
    
    # Test Validation
    print("\n4. Validating Model...")
    validation = model.validate_model()
    print(json.dumps(validation, indent=2))
    
    # Test Attack Flow Simulation
    print("\n5. Testing Attack Flow Simulation...")
    flow_sim = AttackFlowSimulator()
    sim_result = flow_sim.simulate_attack_flow(
        attack_type="FGSM Attack",
        target_device="Camera",
        mitigation_strategy="AI-Powered Detection",
        duration_seconds=120
    )
    print(json.dumps(sim_result, indent=2))
